---
title: "test"
author: "eliasgarzav"
date: "28/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(prophet)
library(dplyr)
library(fpp2)
library(readr)
library(ggplot2)
library(forecast)
library(forecastHybrid)
library(gbm)
library(nnfor)
```

Reading Data
```{r}
Tmatrix <- read_csv("Tobacco_Consumption.csv")
Tdata<-as.data.frame(Tmatrix)
Tdata$item<-paste(Tdata$Submeasure," in ",Tdata$`Data Value Unit`)
```

Dividing by products
```{r}
Products<-list()
for (i in Tdata$item[1:13]){
  Products<-c(Products,list(filter(Tdata, item==i)))
}
names(Products)<-Tdata$item[1:13]
```

Create df of Totals, Imports and Domestic per Capita per Product
```{r}
totalsPerCapita<-Products[[1]]%>%select(11)/Products[[1]]$Population
importsPerCapita<-Products[[1]]%>%select(10)/Products[[1]]$Population
domesticPerCapita<-Products[[1]]%>%select(9)/Products[[1]]$Population
for(j in c(2:13)){
  totalsPerCapita<-cbind(totalsPerCapita,Products[[j]]%>%select(11)/Products[[j]]$Population)
  importsPerCapita<-cbind(importsPerCapita,Products[[j]]%>%select(10)/Products[[j]]$Population)
  domesticPerCapita<-cbind(domesticPerCapita,Products[[j]]%>%select(9)/Products[[j]]$Population)
}
```


```{r}
for(i in c(1:13)){
  print(
      ggplot(data=Products[[i]], aes(x=c(2000:2020))) +
      geom_line(aes(y =totalsPerCapita[[i]],color='Total'))+
      geom_line(aes(y=importsPerCapita[[i]],color='Imports'))+
      geom_line(aes(y=domesticPerCapita[[i]],color='Domestic'))+
      xlab('Years')+ylab('Totals Per Capita')+
      labs(title=names(Products)[i])+
      scale_color_manual(name='Legend',values = c('Total' = "black", "Imports" = "blue",'Domestic'='red')))
}
```

Generating Training and Testing data
```{r}
trainTotals<-list()
testTotals<-list()
for(i in c(1:13)){
  trainTotals<-c(trainTotals, list(ts(head(totalsPerCapita[[i]],17),start=c(2000),end=c(2016),frequency = 1)))
  testTotals<-c(testTotals, list(ts(tail(totalsPerCapita[[i]],4),start=c(2017),end=c(2020),frequency = 1)))
}

```


Auto-ARIMA
```{r}
for(i in c(1:13)){
  sarima_ts<-auto.arima(trainTotals[[i]])
  summary(sarima_ts)
  plot(forecast(sarima_ts,h=5))
}
```

Facebook Prophet Prediction Model
```{r}
dat<-as.character(c(2000:2016))
dates<-c(rep('-01-01',17))
dates<-paste(dat,dates,sep='')
dates<-as.Date.character(dates,format = c('%Y-%m-%d'))

for( p in c(1:13)){
    m <- prophet(data.frame(ds=dates,y=trainTotals[[p]]%>%as.numeric()),changepoint.prior.scale = 0.1)
    future <- make_future_dataframe(m, periods = 5, freq = 60*60*24*366)
    forecast <- predict(m, future)
    plot(m, forecast)
}
```

Neural Network Autoregression
```{r}
for(i in c(1:13)){
  fit<-nnetar(trainTotals[[i]],lambda='auto')
  plot(forecast(fit,h=5))
}
```

Hybrid Model
```{r}
for(i in c(1:13)){
hyb.mod<- hybridModel(trainTotals[[i]])
hyb.f <- forecast(hyb.mod,5)
plot(hyb.f)  
}
```

Multilayer Perceptron Model
```{r}
for(i in c(1:13)){
  mlp.16<-mlp(trainTotals[[i]])
  mlp.16
  mlp.f16<-forecast(mlp.16,5)
  plot(mlp.f16)
}
```

Polinomial Regression
```{r}
for(h in c(1:13)){
  #Load and plot the data
  polydf <- data.frame(year=c(2000:2016),value=trainTotals[[h]]%>%as.numeric())
  
  #randomly shuffle data
  polydf.shuffled <- polydf[sample(nrow(polydf)),]
  
  #define number of folds to use for k-fold cross-validation
  K <- 10 
  
  #define degree of polynomials to fit
  degree <- 5
  
  #create k equal-sized folds
  folds <- cut(seq(1, nrow(polydf.shuffled)) , breaks=K , labels=FALSE)
  
  #create object to hold MSE's of models
  mse = matrix(data=NA,nrow=K,ncol=degree)
  
  #Perform K-fold cross validation
  for(i in 1:K){
      
      #define training and testing data
      testData <- data.frame(year=c(2017:2020),value=testTotals[[h]]%>%as.numeric())
      trainData <- data.frame(year=c(2000:2016),value=trainTotals[[h]]%>%as.numeric())
      
      #use k-fold cv to evaluate models
      for (j in 1:degree){
          fit.train = lm(value ~ poly(year,j), data=trainData)
          fit.test = predict(fit.train, newdata=testData)
          mse[i,j] = mean((fit.test-testData$value)^2) 
      }
  }
  
  #find MSE for each degree 
  mmse =colMeans(mse)
  #determine which is the better degree
  mdegree = which.min(mmse)
  
  # Make predictions
  model <- lm(value ~ poly(year, mdegree), data = polydf)
  predictions <- model %>% predict(data.frame('year'=c(2017, 2021)))
  predictionsdf <- data.frame('year' = c(2017, 2021), 'value' = predictions)
  totaldf <- rbind(polydf, predictionsdf )
  
  print(ggplot(totaldf, aes(x=year, y=value)) + 
            geom_point() +
            stat_smooth(method='lm', formula = y ~ poly(x,mdegree), size = 1)+
            xlab('year') +
            ylab('value'))
}

```

Exponential Smoothing
```{r}
library(smooth)
library(greybox)
for (i in c(1:13)){
  es(trainTotals[[i]], h=4, holdout=FALSE, interval=TRUE, silent=FALSE)
}
```

Simple Moving Average
```{r}
for (i in c(1:13)){
  sma(trainTotals[[i]], h=4, holdout=FALSE, interval=TRUE, silent=FALSE)
}
```

